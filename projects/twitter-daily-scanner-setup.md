# Twitter Daily Scanner - Setup & Integration

## Overview

Automated daily Twitter cryptocurrency scanning system with:
- ‚úÖ Daily scheduled scans (configurable time)
- ‚úÖ Multi-category classification (Airdrop, DeFi, NFT, Security, etc.)
- ‚úÖ Sentiment analysis + ranking
- ‚úÖ Notion database integration
- ‚úÖ Undetectable automation (Camofox + anti-detection)
- ‚úÖ Top N results per category

---

## Components

### 1. TwitterDailyScanner (`twitter_daily_scanner.py`)
Core scanning engine with categorization & ranking.

**Features**:
- 15 default crypto-focused queries
- 10 tweet categories
- Sentiment analysis via Claude API
- Automatic ranking by sentiment + confidence
- Result grouping by category

**Categories**:
- üéÅ **Airdrop**: Free token distributions, claims, eligibility
- üí∞ **DeFi**: Yield farming, protocols, liquidity pools
- üñºÔ∏è **NFT**: Collections, mints, marketplace news
- üí± **Exchange**: Listings, trading, CEX news
- üîë **Wallet**: Security, custody, self-custody
- üó≥Ô∏è **Governance**: DAOs, voting, proposals
- üîí **Security**: Hacks, exploits, audits
- üìä **Price**: Charts, technicals, predictions
- üöÄ **Adoption**: Mainstream news, partnerships
- ‚öñÔ∏è **Regulation**: Legal, policy, compliance

### 2. TwitterNotionSync (`twitter_notion_sync.py`)
Pushes results to Notion database.

**Features**:
- Create/update Notion pages
- Search for existing pages
- Batch sync operations
- Category parent pages
- Emoji-labeled entries

### 3. TwitterDailyJob (`twitter_daily_job.py`)
Scheduler for automatic daily execution.

**Features**:
- Background scheduler (APScheduler)
- Configurable daily time
- Status monitoring
- Manual run capability
- Grace period for missed runs

---

## Setup

### Prerequisites

```bash
# Install dependencies
pip install anthropic requests apscheduler

# Environment variables
export NOTION_API_KEY="your-notion-api-key"
export NOTION_DATABASE_ID="your-notion-db-id"
export CAMOFOX_URL="http://localhost:9377"  # Optional
```

### Notion Database Setup

1. **Create database** in Notion
2. **Add properties**:
   - `title` (Text)
   - `Category` (Select: Airdrop, DeFi, NFT, Exchange, Wallet, Governance, Security, Price, Adoption, Regulation)
   - `Sentiment` (Select: Very Bullish, Bullish, Neutral, Bearish, Very Bearish)
   - `Score` (Number)
   - `Confidence` (Number)
   - `Rank` (Number)
   - `Tweets` (Number)
   - `Themes` (Text)
   - `Last Updated` (Date)

3. **Get API key**:
   - Go to https://www.notion.so/my-integrations
   - Create new integration
   - Copy API key ‚Üí `NOTION_API_KEY`

4. **Get database ID**:
   - Open Notion database
   - Get ID from URL: `notion.so/workspace/**[db-id]**?v=...`
   - Copy ‚Üí `NOTION_DATABASE_ID`

### Docker Setup

```bash
# Start required services
docker-compose up -d camofox postgres redis

# Verify Camofox
curl http://localhost:9377/health
```

---

## Usage

### Basic Setup

```python
from oracle.core.twitter_daily_job import TwitterDailyJob

# Create job (scan at 9:00 UTC daily)
job = TwitterDailyJob(scan_time="09:00")

# Start scheduler
job.start()

# Check status
print(job.get_status())
# {
#     "running": True,
#     "scheduled_time": "09:00",
#     "last_run": None,
#     "last_result_count": 0,
#     "notion_enabled": True,
#     "next_run": "2026-02-13T09:00:00"
# }

# Run immediately (for testing)
job.run_now()

# Stop
job.stop()
```

### Custom Queries

```python
from oracle.core.twitter_daily_scanner import TwitterDailyScanner

scanner = TwitterDailyScanner()

# Custom queries
queries = [
    "solana ecosystem growth",
    "arbitrum ecosystem activity",
    "polygon adoption news"
]

# Scan
results = scanner.scan_daily(queries)

# Get top 3 per category
top = scanner.get_top_by_category(results, top_n=3)

# Format for Notion
notion_data = scanner.format_for_notion(top)
```

### Manual Notion Sync

```python
from oracle.core.twitter_notion_sync import TwitterNotionSync

sync = TwitterNotionSync()

# Sync results
stats = sync.sync_batch(results)
print(f"Created: {stats['created']}, Updated: {stats['updated']}")
```

### Integration with Cron

```python
# scripts/run_twitter_scan.py
from oracle.core.twitter_daily_job import start_twitter_job

# Start daily scan at 9:00 UTC
job = start_twitter_job(scan_time="09:00")

# Keep running
import time
while True:
    time.sleep(60)
```

```bash
# Run via cron
0 9 * * * cd /path/to/oracle && python scripts/run_twitter_scan.py
```

---

## Configuration

### Scan Time Options

```python
# Morning (Europe)
job = TwitterDailyJob(scan_time="08:00")  # 8:00 UTC

# Noon
job = TwitterDailyJob(scan_time="12:00")  # 12:00 UTC

# Evening (US East)
job = TwitterDailyJob(scan_time="18:00")  # 18:00 UTC (2 PM EST)

# Late night
job = TwitterDailyJob(scan_time="23:00")  # 23:00 UTC
```

### Query Customization

```python
# Default queries (15 total)
results = scanner.scan_daily()

# Custom queries only
results = scanner.scan_daily(
    queries=["my query 1", "my query 2"],
    max_tweets_per_query=100
)
```

### Ranking Weights

```python
# Customize ranking
rank_score = ResultRanker.calculate_rank_score(
    sentiment_score=0.7,      # -1.0 to +1.0
    confidence=0.85,           # 0.0 to 1.0
    tweet_count=50,            # Number of tweets
    category_rarity=1.5        # Weight for category (default: 1.0)
)
```

---

## API Reference

### TwitterDailyScanner

```python
scanner = TwitterDailyScanner(
    camofox_url="http://localhost:9377",
    proxy_list=["proxy1", "proxy2"]  # Optional
)

# Scan with default queries
results = scanner.scan_daily()

# Scan with custom queries
results = scanner.scan_daily(
    queries=["query1", "query2"],
    max_tweets_per_query=50
)

# Get top results per category
top = scanner.get_top_by_category(results, top_n=3)

# Format for Notion
notion_data = scanner.format_for_notion(top)

# Close pool
scanner.close()
```

### TwitterNotionSync

```python
sync = TwitterNotionSync(
    notion_api_key="ntn_...",
    database_id="abcd1234..."
)

# Create or update single page
page_id = sync.create_or_update_page(result)

# Batch sync
stats = sync.sync_batch(results)

# Create category pages
cats = sync.create_category_pages()

# Find existing page
page_id = sync.find_existing_page(query, category)
```

### TwitterDailyJob

```python
job = TwitterDailyJob(
    camofox_url="http://localhost:9377",
    proxy_list=["proxy1", "proxy2"],
    notion_api_key="ntn_...",
    notion_database_id="abcd1234...",
    scan_time="09:00"
)

# Lifecycle
job.start()
job.run_now()
job.stop()

# Status
status = job.get_status()
jobs = job.get_jobs()

# Last run info
print(job.last_run)
print(job.last_result_count)
```

---

## Example Output

### Scan Results
```python
{
    TweetCategory.AIRDROP: [
        CategorizedResult(
            query="airdrop distribution",
            sentiment_score=0.75,
            confidence=0.85,
            key_themes=["claim", "eligibility"],
            rank_score=75.3,
            tweet_count=45
        ),
        ...
    ],
    TweetCategory.DEFI: [
        CategorizedResult(
            query="defi yield farming",
            sentiment_score=0.6,
            confidence=0.8,
            key_themes=["liquidity", "aave"],
            rank_score=62.1,
            tweet_count=38
        ),
        ...
    ]
}
```

### Notion Page Example

| Title | Category | Sentiment | Score | Confidence | Rank | Tweets | Themes | Updated |
|-------|----------|-----------|-------|------------|------|--------|--------|---------|
| üéÅ airdrop distribution | Airdrop | Bullish | 0.75 | 0.85 | 75.3 | 45 | claim, eligibility | 2026-02-12 |
| üí∞ defi yield farming | DeFi | Bullish | 0.60 | 0.80 | 62.1 | 38 | liquidity, aave | 2026-02-12 |

---

## Troubleshooting

### No results found
```python
# Check queries
print(TwitterDailyScanner.DEFAULT_QUERIES)

# Use custom queries
results = scanner.scan_daily(
    queries=["bitcoin", "ethereum"]
)

# Increase max_tweets
results = scanner.scan_daily(max_tweets_per_query=100)
```

### Notion sync fails
```python
# Verify credentials
import os
api_key = os.getenv("NOTION_API_KEY")
db_id = os.getenv("NOTION_DATABASE_ID")

# Test connection
sync = TwitterNotionSync(api_key, db_id)
pages = sync.find_existing_page("test", TweetCategory.AIRDROP)
```

### Scheduler not running
```python
# Check status
job = TwitterDailyJob()
print(job.get_status())

# Restart
job.stop()
job.start()

# View jobs
print(job.get_jobs())
```

### Low confidence results
```python
# Adjust classification
category, confidence = classifier.classify(query, themes)
if confidence < 0.5:
    logger.warning(f"Low confidence: {confidence}")

# Add more keywords to CATEGORY_KEYWORDS
```

---

## Performance

### Metrics
- **Scan time**: ~5-10 seconds per query
- **Notion sync**: ~1 second per page
- **Total daily run**: ~2-3 minutes (15 queries)
- **API calls**: 15 Twitter scans + N Notion syncs
- **Token usage**: ~1.25M tokens/day @ 15 queries

### Cost
- **Twitter scanning**: ~$3.75/month (99% reduction via Camofox)
- **Notion**: Free (API calls)
- **Claude API**: ~$0.50/month

---

## Testing

```bash
# Run tests
pytest oracle/tests/test_twitter_daily_scanner.py -v

# Run with coverage
pytest oracle/tests/test_twitter_daily_scanner.py --cov=oracle.core.twitter_daily_scanner

# Test individual components
pytest oracle/tests/test_twitter_daily_scanner.py::TestCategoryClassifier -v
pytest oracle/tests/test_twitter_daily_scanner.py::TestResultRanker -v
```

---

## Integration with Other Systems

### With Telegram Bot
```python
from oracle.core.telegram_bot import TelegramBot

bot = TelegramBot()

# Send top results to Telegram
for category, results in top.items():
    for result in results:
        message = f"üîî {category.value}: {result.query}\n"
        message += f"Sentiment: {result.sentiment_score:.2f}\n"
        message += f"Themes: {', '.join(result.key_themes)}"
        
        bot.send_message(message)
```

### With Database
```python
from oracle.core.database import save_twitter_scan

# Save results
for category, results in by_category.items():
    for result in results:
        save_twitter_scan({
            "query": result.query,
            "category": category.value,
            "sentiment": result.sentiment_score,
            "confidence": result.confidence,
            "rank": result.rank_score,
            "timestamp": result.detected_at
        })
```

### With Alerts
```python
# Alert on high-score results
for category, results in by_category.items():
    for result in results:
        if result.rank_score > 80:
            send_alert(f"üöÄ High-confidence {category.value}: {result.query}")
```

---

## Deployment

### Docker
```dockerfile
FROM python:3.9

WORKDIR /oracle

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY oracle ./oracle

CMD ["python", "-m", "oracle.core.twitter_daily_job"]
```

### Kubernetes
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: twitter-daily-scan
spec:
  schedule: "0 9 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: scanner
            image: oracle:latest
            env:
            - name: NOTION_API_KEY
              valueFrom:
                secretKeyRef:
                  name: oracle-secrets
                  key: notion_api_key
            - name: NOTION_DATABASE_ID
              valueFrom:
                secretKeyRef:
                  name: oracle-secrets
                  key: notion_database_id
          restartPolicy: OnFailure
```

---

## Monitoring

### Log Files
```bash
# View logs
tail -f oracle.log

# Filter by scanner
grep "Twitter daily scan" oracle.log

# Check errors
grep "ERROR" oracle.log
```

### Metrics
```python
# Job statistics
job = TwitterDailyJob()
status = job.get_status()

# Result metrics
print(f"Last run: {status['last_run']}")
print(f"Results: {status['last_result_count']}")
print(f"Next run: {status['next_run']}")
```

---

**Status**: ‚úÖ Ready for deployment  
**Last updated**: 2026-02-12 14:20 GMT+1  
**Daily cost**: ~$4.25/month (99% reduction)  
**Categories**: 10 + customizable queries
